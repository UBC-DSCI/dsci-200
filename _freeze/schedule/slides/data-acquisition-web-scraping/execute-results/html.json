{
  "hash": "a590bbe9c2e32f3df14e5145d35ffdae",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"Data Acquisition via Web Scraping\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"img/smooths.png\" background-opacity=\"0.3\" background-size=\"50%\"}\n\n[DSCI 200]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 27 February 2026\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## Attribution\n<br><br>\n\n*This material is adapted from the following sources:* \n\n- [*R for Data Science (2e): Chapter 24 - Web Scraping*](https://r4ds.hadley.nz/webscraping) \n- [*STA 199: Introduction to Data Science and Statistical Thinking*](https://sta199-s24.github.io/), *Mine Çetinkaya-Rundel, Duke University.*\n- *Dogucu, M., & Çetinkaya-Rundel, M. (2020). Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities. Journal of Statistics and Data Science Education, 29(sup1), S112–S122.* [*https://doi.org/10.1080/10691898.2020.1787116*](https://doi.org/10.1080/10691898.2020.1787116)\n\n<br><br>\n\n\n## Web Scraping\n<br></br>\n\n:::incremental\n- Web scraping is a powerful tool for extracting data from web pages.\n- As we discussed previously, some websites provide APIs, in which case they should be used as the data obtained is likely more reliable.\n- However, web scraping is useful when no API is available.\n:::\n\n## Learning Objectives\n<br></br>\n\nBy the end of today's lesson, you should be able to:\n\n:::incremental\n- Explain the ethics and legal considerations of web scraping.\n- Identify CSS selectors to locate specific elements on a web page.  \n- Utilize the `rvest`  package to extract data from text and HTML attributes into R.  \n- Determine and test the appropriate CSS selector needed for scraping a given web page.  \n:::\n\n## Packages\n\n\n- We’ll use the ` rvest` package, part of the `tidyverse` (but not core).\n- We will also use the `robotstxt` package to verify whether we *can* scrape data from a particular url. \n\n<div style=\"text-align: center;\">\n  <img src=\"https://rvest.tidyverse.org/logo.png\" style=\"width: 20%;\" />\n</div>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(robotstxt)\n```\n:::\n\n\n## \n<br></br>\n\nMany websites present their content (including data) using HyperText Markup Language (HTML).\n\n:::{.columns .justify-center}\n::: {.column width=\"45%\"}\n![](img/lakelouise.png)\n:::\n\n::: {.column width=\"45%\"}\n![](img/html.png)\n:::\n:::\n\n<div style=\"text-align: center; font-size: 0.9em; margin-top: 1em;\">\nSource: <a href=\"https://parks.canada.ca/pn-np/ab/banff/activ/randonnee-hiking/lakelouise\" target=\"_blank\">https://parks.canada.ca/pn-np/ab/banff/activ/randonnee-hiking/lakelouise</a>\n</div>\n\n\n\n\n## Introduction to HTML\n\n- Web scraping requires a basic understanding of HTML, which structures web pages.\n- Example of a simple HTML structure:\n\n```html\n<html>\n  <head>\n    <meta charset=\"UTF-8\">\n    <title>Sample Webpage</title>\n  </head>\n  <body>\n    <h2 class=\"main-title\">Welcome to My Site!</h2>\n    <p>Here is some sample text with <em>emphasis</em> included.</p>\n    <img src=\"sample-image.png\" alt=\"A descriptive image\" width=\"200\">\n  </body>\n</html>\n```\n- While this course doesn't aim to teach HTML in depth, having a basic understanding is useful for getting started with web scraping.\n\n## HTML Structure\n<br></br>\n\n- HTML consists of **elements** with:\n  - **Start tag** (e.g., `<p>`) \n  - **Content** (text or other elements)\n  - **End tag** (e.g., `</p>`)\n- Some tags can contain other elements, forming a **hierarchical structure**.\n\n---\n\n\n## Essential HTML Elements\n<br></br>\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n### **Document Structure**  \n- `<html>`: The root element.  \n- `<head>`: Metadata like title and styles.  \n- `<body>`: The visible content.  \n:::\n\n::: {.column width=\"30%\"}\n### **Block Elements**  \n- `<h1>` - Headings  \n- `<p>` - Paragraphs  \n- `<section>` - Sections  \n- `<ol>` - Ordered lists  \n:::\n\n::: {.column width=\"30%\"}\n### **Inline Elements**  \n- `<b>` - Bold  \n- `<i>` - Italics  \n- `<a>` - Links  \n:::\n\n:::: \n\n---\n\n## Understanding HTML Elements\n<br></br>\n\n- Elements may contain **child elements**.\n- Example:\n\n```html\n<p>\n  Hello! My <b>name</b> is Katie.\n</p>\n```\n\n- Here, `<p>` is the parent, `<b>` is the child\n- Note that `<b>` has no children, but it has the content \"name\"\n\n---\n\n## HTML Attributes\n<br></br>\n\n- Attributes provide extra information about elements.\n- Example syntax:\n  ```html\n  <a href=\"https://ubc-stat.github.io/dsci-200/\">Click here</a>\n  ```  \n- `<a>` is the tag (HTML element) for a link.\n- href is the attribute of the `<a>` tag, which specifies the URL the link points to.\n- \"https://ubc-stat.github.io/dsci-200/\" is the value of the href attribute, indicating the destination of the link.\n- [`Click here`](https://ubc-stat.github.io/dsci-200) is the content inside the tag, which the user will see and click.\n\n## Important attributes\n<br></br>\n\n  - **id**: Unique identifier (e.g., `id='header'`)\n  - **class**: Categorizes elements (e.g., `class='nav-item'`)\n  - **href**: Specifies link destinations (`<a href='example.com'>`)\n  - **src**: Image sources (`<img src='photo.jpg'>`)\n<br></br>\n\n> id and class are used with CSS (Cascading Style Sheets) to control the appearance of a page and are often useful when web scraping data!\n\n\n\n## Important `rvest` functions \n\n:::incremental\n- `read_html()`: Loads HTML content from a webpage URL or a character string.\n\n- `html_element()`: Retrieve a single matching element based on a CSS selector.\n\n- `html_elements()`: Retrieves all matching elements using CSS selectors.\n\n- `html_table()`: Converts HTML tables into data frames.\n\n- `html_text()`/`html_text2()`: Gets the text content from within HTML tags.\n\n- `html_name()`: Returns the tag name(s) of HTML elements.\n\n- `html_attr()`: Retrieves a single attribute.\n\n- `html_attrs()`: Retrieves all attributes. \n:::\n\n## Sample HTML  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhtml <- \n'<html>\n  <head>\n    <title>Sample Webpage</title>\n  </head>\n  <body>\n    <h2 class=\"sub-title\">Welcome to My Site!</h2>\n    <p>Here is some sample text.</p>\n    <p> Some useful information... </p>\n    <img src=\"sample-image.png\" alt=\"A descriptive image\" width=\"200\">\n  </body>\n</html>'\n\nread_html(html)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{html_document}\n<html>\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n    <h2 class=\"sub-title\">Welcome to My Site!</h2>\\n    <p>Here i ...\n```\n\n\n:::\n:::\n\n\n## Selecting elements\n\n\n- `html_element()` always returns the same number of outputs as inputs. If you apply it to a whole document it’ll give you the first match:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nread_html(html) |> html_element(\"head\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{html_node}\n<head>\n[1] <title>Sample Webpage</title>\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nread_html(html) |> html_elements(\"p\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (2)}\n[1] <p>Here is some sample text.</p>\n[2] <p> Some useful information... </p>\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Extracting Text & Attributes\n<br></br>\n\n- Use `html_text()` or `html_text2()` to get **text content**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nread_html(html) |> html_element(\"head\") |> html_text()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Sample Webpage\"\n```\n\n\n:::\n:::\n\n\n\n\n- You can use `html_attr()` to extract **attributes** like links:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhtml <- minimal_html(\"\n  <p><a href='https://wiki.ubc.ca/Map_of_Coffee_Places_on_Campus'>UBC coffee map</a></p>\n\")\n\t\nhtml |> \n  html_elements(\"p\") |> \n  html_element(\"a\") |> \n  html_attr(\"href\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"https://wiki.ubc.ca/Map_of_Coffee_Places_on_Campus\"\n```\n\n\n:::\n:::\n\n\n\n\n## Nesting Selections\n<br></br>\n\n- In most cases, you'll use `html_elements()` and `html_element()` together.  \n- Typically, `html_elements()` identifies elements that will become **observations**, and `html_element()` extracts **variables** from those elements. Here’s an example using a simple HTML list of coffee shops around UBC:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhtml <- read_html(\"\n  <ul>\n    <li><b>Loafe Cafe</b> serves <i>coffee and pastries</i> and has <span class='seating'>indoor & outdoor seating</span></li>\n    <li><b>Bean Around the World</b> serves <i>great coffee</i></li>\n    <li><b>JJ Bean</b> serves <i>strong espresso</i> and has <span class='seating'>cozy indoor seating</span></li>\n    <li><b>The Great Dane</b> has <span class='seating'>a dog-friendly patio</span></li>\n  </ul>\n  \")\n```\n:::\n\n\n\n## Extracting Coffee Shop Names\n<br></br>\n\nWe use `html_elements(\"li\")` to create a vector where each element represents a different coffee shop:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshops <- html |> html_elements(\"li\")\nshops\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (4)}\n[1] <li>\\n<b>Loafe Cafe</b> serves <i>coffee and pastries</i> and has <span c ...\n[2] <li>\\n<b>Bean Around the World</b> serves <i>great coffee</i>\\n</li>\n[3] <li>\\n<b>JJ Bean</b> serves <i>strong espresso</i> and has <span class=\"s ...\n[4] <li>\\n<b>The Great Dane</b> has <span class=\"seating\">a dog-friendly pati ...\n```\n\n\n:::\n:::\n\n\nTo extract the name of each shop, we use `html_element(\"b\")`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshops |> html_element(\"b\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (4)}\n[1] <b>Loafe Cafe</b>\n[2] <b>Bean Around the World</b>\n[3] <b>JJ Bean</b>\n[4] <b>The Great Dane</b>\n```\n\n\n:::\n:::\n\n\n\n## Extracting Seating Information\n<br></br>\n\nSuppose we want one seating description for each shop, even if some don't have seating information.  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshops |> html_element(\".seating\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (4)}\n[1] <span class=\"seating\">indoor &amp; outdoor seating</span>\n[2] NA\n[3] <span class=\"seating\">cozy indoor seating</span>\n[4] <span class=\"seating\">a dog-friendly patio</span>\n```\n\n\n:::\n:::\n\n\n##\n<br></br>\n\nWhat if we used `html_elements(\".seating\")`?\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshops |> html_elements(\".seating\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (3)}\n[1] <span class=\"seating\">indoor &amp; outdoor seating</span>\n[2] <span class=\"seating\">cozy indoor seating</span>\n[3] <span class=\"seating\">a dog-friendly patio</span>\n```\n\n\n:::\n:::\n\n\n\n## HTML Tables\n<br></br>\n\nHTML tables are often used to store tabular data on web pages and you can easily extract it. Key HTML table elements:\n\n- `<table>`: Defines the table.\n- `<tr>`: Table row.\n- `<th>`: Table heading.\n- `<td>`: Table data.\n\n## Example of an HTML Table\n<br></br>\n\nHere’s a simple HTML table with two columns and three rows:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhtml <- read_html(\"\n  <table class='mytable'>\n    <tr><th>x</th>   <th>y</th></tr>\n    <tr><td>1.5</td> <td>2.7</td></tr>\n    <tr><td>4.9</td> <td>1.3</td></tr>\n    <tr><td>7.2</td> <td>8.1</td></tr>\n  </table>\n  \")\n```\n:::\n\n\n\n## Extracting Tables\n<br></br>\n\n- You can use the `rvest` package to extract tables from HTML pages:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhtml |> \n  html_element(\".mytable\") |> \n  html_table()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n      x     y\n  <dbl> <dbl>\n1   1.5   2.7\n2   4.9   1.3\n3   7.2   8.1\n```\n\n\n:::\n:::\n\n\n\n- The `html_table()` function automatically converts values like \"x\" and \"y\" to numbers.\n\n- If you want to prevent this automatic conversion and handle it yourself, use the `convert = FALSE` argument.\n\n## CSS Selectors\n<br></br>\n\nCSS (Cascading Style Sheets) helps define page structure and styling.\nTo extract elements efficiently, we use **selectors** such as:\n<br></br>\n\n- `p` → selects all `<p>` elements\n- `.title` → selects elements with class \"title\"\n- `#title` → selects element with ID \"title\"\n\nFinding the right CSS selector is typically the hardest part of web scraping. This is because you want a selector that is specific enough that you aren't capturing unnecessary information, but you also don't want your scope to be too narrow such that you miss important information.\n\n\n\n## SelectorGadget\n<br></br>\n\n\n- Instead of inspecting raw HTML code to find the right CSS selector, we are going to use a tool called [SelectorGadget](https://selectorgadget.com/).\n\n- SelectorGadget works best with Chrome browsers. If you haven't done so already, [install the Chrome extension here](https://chromewebstore.google.com/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en).\n\n- Once added, the icon should appear to the right of the search bar.\n\n![](img/selectorgadget.png)\n\n##\n\n::: {style=\"text-align: center; margin: 0; padding: 0;\"}\n<img src=\"img/selector.gif\" style=\"width: 75%; margin: 0; padding: 0;\"/>\n:::\n\n## In-class Exercise\n\n- You're planning an adventurous getaway to the stunning wilderness of Lake Louise in Banff National Park, Alberta. To make the most of your trip, you want to gather up-to-date information on the best hiking trails in the area with details like trail distance and elevation gain. \n\n- In this exercise, you'll explore a real-world data collection task by scraping hiking trail information from Parks Canada.\n\n\n## The Data\n<br></br>\n\n- Let's explore the Park's Canada website that contains information about day hikes in the Lake Louise area:\n\n[https://parks.canada.ca/pn-np/ab/banff/activ/randonnee-hiking/lakelouise](https://parks.canada.ca/pn-np/ab/banff/activ/randonnee-hiking/lakelouise)\n\n## \n\n- We first need to verify that this website *can* be scraped (note this is very different than whether the website *should* be scraped from an ethical or legal perspective).\n- To do this, we can use the `paths_allowed()` function from the `robotstxt` package. This function checks if a bot has permissions to access page(s) and returns `TRUE` if allowed based on the site's `robots.txt` file. \n\n<br></br>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npaths_allowed(\"https://parks.canada.ca/pn-np/ab/banff/activ/randonnee-hiking/lakelouise\", warn=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n## Load and Read HTML Page\n\n- Let's use `rvest` to read HTML content from Parks Canada site  \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npage <- read_html(\"https://parks.canada.ca/pn-np/ab/banff/activ/randonnee-hiking/lakelouise\")\n\npage\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{html_document}\n<html class=\"no-js\" lang=\"en\" dir=\"ltr\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body class=\"\" vocab=\"http://schema.org/\" typeof=\"WebPage\">\\r\\n    \\r\\n\\r ...\n```\n\n\n:::\n:::\n\n\n\n## Extract Trail Information\n\n\n- Use SelectorGadget to obtain the CSS selectors to locate the trail names, the distance and elevation gain. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrails <- page |>\n  html_elements(\"td:nth-child(1)\") |>\n  html_text()\n\ndistances <- page |>\n  html_elements(\"td:nth-child(2)\") |>\n  html_text()\n\nelevations <- page |>\n  html_elements(\"td:nth-child(4)\") |>\n  html_text()\n```\n:::\n\n\n\n\n## Create a data frame\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhikes_raw <- tibble(\n  trail = trails,\n  distance = distances,\n  elevation = elevations\n)\n\nhead(hikes_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  trail                  distance     elevation\n  <chr>                  <chr>        <chr>    \n1 Lake Louise Lakeshore  2.3 km       minimal  \n2 Fairview Lookout       1.2 km       100 m    \n3 Bow River              Up to 5.7 km minimal  \n4  Rockpile              0.7 km loop  35 m     \n5 Moraine Lake Lakeshore 1.3 km       minimal  \n6 Consolation Lakes      2.9 km       135 m    \n```\n\n\n:::\n:::\n\n\n\n## Clean data\n\n:::incremental\n- GOALS: \n\t- Extract numeric values from text fields\n\t- Handle values of \"minimal\" for elevation\n\t- Convert distances to numeric (in km) in a consistent format\n\n- To achieve these goals, we need to do some string manipulation using the `stringr` package. This is not a specific learning objective of this course, but some string manipulation may be needed depending on the data. Please consult your instructor or a TA for any additional guidance on working with strings and regular expressions in R. \n:::\n\n## \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(stringr)\n\nhikes_clean <- hikes_raw |>\n  mutate(\n    distance_km = str_extract(distance, \"\\\\d+(\\\\.\\\\d+)?\") |> as.numeric(),\n    elevation_m = case_when(\n      str_detect(elevation, \"minimal\") ~ 0,\n      TRUE ~ str_extract(elevation, \"\\\\d+\") |> as.numeric()\n    )\n  ) |>\n  select(-distance, -elevation)\n\nhead(hikes_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  trail                  distance_km elevation_m\n  <chr>                        <dbl>       <dbl>\n1 Lake Louise Lakeshore          2.3           0\n2 Fairview Lookout               1.2         100\n3 Bow River                      5.7           0\n4  Rockpile                      0.7          35\n5 Moraine Lake Lakeshore         1.3           0\n6 Consolation Lakes              2.9         135\n```\n\n\n:::\n:::\n\n\n\n## \n\nWhat are some of the longest day hikes in the Lake Louise area? Now that the data is in a tidy format, we are free to explore! \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(hikes_clean, aes(x = reorder(trail, distance_km), y = distance_km)) +\n  geom_col(fill = \"#0570b0\") +\n  coord_flip() +\n  labs(\n    title = \"Bar Chart of Distance by Trail\",\n    x = \"Trail\",\n    y = \"Distance (km)\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](data-acquisition-web-scraping_files/figure-revealjs/unnamed-chunk-19-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## A Note about Workflows\n\n:::incremental\n- When working in a Jupyter notebook, everytime we click \"Restart and Run All Cells\" our entire analysis is rerun. \n- If we are web scraping, this means that each time we run our notebook we will be scraping new data, which isn't ideal since:\n\t- a) the data could have changed since the first time we wrote our analysis (not good for reproducibility)\n\t- b) it puts additional strain on the particular web server\n- A suggested workflow is to use a script to save your code, save your scraped data as a CSV using `write_csv()` and use the saved data in your analysis.\n:::\n\n\n## Scraping Ethics \n<br></br>\n\n- Web scraping has both ethical and legal considerations, which can vary depending on your location. \n\n- We will discuss data ownership in more detail later in the course, but for now we will discuss some general guidelines.\n<br></br>\n\n> *Note: While we are providing general guidelines and best practices for web scraping, please note that we are not lawyers. If you are unsure about the legal or ethical implications of a specific scraping project, you should consult a qualified legal professional. Always prioritize respecting website terms, data privacy, and intellectual property rights.*\n\n## Scraping Ethics \n<br></br>\n**General Guidelines**: \n\n:::incremental\n- Data should be public, non-personal, and factual to minimize legal risks.  \n- Avoid scraping to profit from the data unless legally cleared.  \n- Be respectful of server resources by pausing between requests. \n- If unsure, consult a legal professional.\n:::\n\n## Terms of Service\n<br></br>\n\n:::incremental\n- Many websites have \"Terms of Service\" that often prohibit scraping.  \n- **Key points to consider**:  \n  - In the US, you are generally not bound by terms of service unless you explicitly agree (e.g., by creating an account or checking a box).  \n  - In Europe, terms of service may be enforceable even without explicit agreement.  \n  - Respect these terms where possible but know they may not always be legally binding.  \n:::\n\n## In-Class Exercise: Reviewing Terms of Service\n<br></br>\n\nIn this exercise, we’ll explore the **Terms of Service** on a website. Your task:\n\n  - Visit a website of your choice (you could use the Parks Canada website).\n  - Look for the \"Terms of Service\" or \"Terms and Conditions\" link (often found in the footer).\n  - Check if there are any explicit mentions of scraping or data usage.\n  - Discuss the terms with the people around you. Try and answer the following questions:  \n    - Is web scraping prohibited?  \n    - Are there any clauses about data usage or copyright?\n    \nWe will share our findings and discuss how these terms might affect web scraping.\n\n## In-Class Exercise: Reviewing Terms of Service\n<br></br>\nAs a class, let's take a look at [Spotify's User Guidelines](https://www.spotify.com/ca-en/legal/user-guidelines/). \n\n<img src=https://storage.googleapis.com/pr-newsroom-wp/1/2018/11/Spotify_Logo_RGB_Green.png width=\"300\">\n\n## Personally Identifiable Information \n<br></br>\n\n- Avoid scraping personally identifiable information such as names, emails, phone numbers, or birthdays.  \n- Releasing public but identifiable information can cause harm.\n- For example, you can read about the legal battle of [hiQ vs. LinkedIn](https://blog.apify.com/hiq-v-linkedin/), in which hiQ Labs used web scraping to collect public data from LinkedIn profiles.\n\n\n## Key Takeaways  \n<br></br>\n\n- Recognizing legal and ethical considerations are critical when scraping data from the web  \n- Implementing the `rvest` package for scraping and parsing HTML content  \n- Understanding the purpose of HTML and CSS selectors when extracting data from web pages  \n- Just because a website *can* be scraped, doesn't mean we should! \n\n",
    "supporting": [
      "data-acquisition-web-scraping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}