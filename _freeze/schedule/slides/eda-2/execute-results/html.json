{
  "hash": "56d83a67d2ece21a8b4ace12d78bc803",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"Exploratory Data Analysis\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"img/smooths.png\" background-opacity=\"0.3\" background-size=\"50%\"}\n\n[DSCI 200]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 02 January 2026\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n\n\n\n## Attribution\n<br><br>\n\nThis material is adapted from the following sources:\n\n- [*Chapter 10 of R for Data Science (2e)*](https://r4ds.hadley.nz/EDA.html) \n- [*Exploratory Data Analysis with R*](https://bookdown.org/rdpeng/exdata/).\n- [*The Simpson's Paradox*](https://www.data-to-viz.com/caveat/simpson.html)\n- [https://www.natekratzer.com/posts/simpsons_paradox/](https://www.natekratzer.com/posts/simpsons_paradox/)\n- [*Data Science with R*](https://kirenz.github.io/data-science-r/docs/data.html)\n\n## Learning Objectives\n\nBy the end of this lesson, you will be able to:\n\n:::incremental\n- Investigate relationships between variables using correlation\n\n- Examine the limitations of Pearson‚Äôs correlation and recognize scenarios where it can be misleading, including Simpson‚Äôs Paradox\n\n- Discuss how choices made during EDA can impact the entire data analysis pipeline\n\n- Recognize the role of data splitting in preventing bias and ensuring generalizability\n:::\n\n## Review\n\n- Last class we recalled the usage of scatterplots as a useful way to visualize the relationship between two numerical variables. \n\n- However, it would be nice to be able to quantify this relationship with some measure of covariation...\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## What is Correlation?\n\n:::incremental\n- Correlation measures the strength and direction of a relationship between variables\n- For example, correlation analysis could help us explore the following questions:\n\t- Is there a relationship between social media usage and mental health scores among teenagers?\n\t- Do housing prices correlate with interest rates in different regions?\n\t- Does the frequency of push notifications correlate with user engagement in a mobile app?\n:::\n\n## Pearson Correlation\n\n- Pearson correlation is perhaps the most common, and it measures the strength of a linear relationship between two numeric variables. \n- Values range from -1 to 1:\n  - 1: perfect positive linear relationship\n  - -1: perfect negative linear relationship\n  - 0: no linear relationship\n\n## Pearson Correlation\n\nFor two variables $X$ and $Y$, the estimated pearson correlation coefficient, $r$, is given by the following formula: \n\n$$ r = \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2(y_i-\\bar{y})^2}},$$\n\nwhere \n\n- $n$ is the sample size\n- $x_i$ and $y_i$ are the $i^{th}$ observations of $X$ and $Y$, respectively\n- $\\bar{x}$ and $\\bar{y}$ are the estimated sample means of $X$ and $Y$, respectively\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n  \n  \n## Guess the Correlation\n  \n  \nTake a minute and play a few rounds of \"[Guess the Correlation](https://www.guessthecorrelation.com/)\" to get a better idea about the strength of different correlations.\n\n![](img/guess-cor.png)\n\n## Load packages\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(corrr)\n```\n:::\n\n\n\n## Computing correlations in R \n- Luckily, there are functions in R that compute correlations for us! \n- While there is a default function `cor()` in base R, the `correlate()` function in the `corrr` package integrates more smoothly with our tidy workflows. \n- Let's work with `airquality` data from the `datasets` pacakge.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(airquality)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n```\n\n\n:::\n:::\n\n\n\n## \n\n- Let's investigate the relationship between mean ozone (ppb) from 1-3pm and maximum daily temperature in degrees Fahrenheit.\n- We can see a moderately strong positive relationship.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(airquality, aes(x = Ozone, y = Temp)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Compute correlation\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nairquality |> \n\tselect(Ozone, Temp) |>\n\tcorrelate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 √ó 3\n  term   Ozone   Temp\n  <chr>  <dbl>  <dbl>\n1 Ozone NA      0.698\n2 Temp   0.698 NA    \n```\n\n\n:::\n:::\n\n\n\n> *Note that the diagonal values are printed as `NA`, but the correlation between a variable and itself is simply 1.*\n\n## \n\nTo just extract the single correlation value: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nairquality |> \n\tselect(Ozone, Temp) |>\n\tcorrelate() |> \n\tfilter(term == \"Ozone\") |>\n\tpull(Temp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6983603\n```\n\n\n:::\n:::\n\n\n\n\n\n## Correlation matrices\n\n- Sometimes, we want to calculate the correlations among multiple numeric variables at once.\n- In such cases, a correlation matrix is useful for exploring the relationships between several variables simultaneously.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nairquality |> \n\tselect(Ozone,Solar.R,Temp,Wind) |>\n  correlate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 √ó 5\n  term     Ozone Solar.R   Temp    Wind\n  <chr>    <dbl>   <dbl>  <dbl>   <dbl>\n1 Ozone   NA      0.348   0.698 -0.602 \n2 Solar.R  0.348 NA       0.276 -0.0568\n3 Temp     0.698  0.276  NA     -0.458 \n4 Wind    -0.602 -0.0568 -0.458 NA     \n```\n\n\n:::\n:::\n\n\n\n\n\n## Visualizing Correlation\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncorrelations <- airquality |> \nselect(Ozone,Solar.R,Temp,Wind) |>\ncorrelate() |>\nrearrange() |> \nshave() \n\nrplot(correlations)\n```\n\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-10-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Limitations of Pearson Correlation\n\n- Pearson $r$ measures **linear** relationships only.\n- It can be 0 even when a strong nonlinear relationship exists!\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-11-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Limitations of Pearson Correlation\n\n:::incremental\n- Assumes both variables are continuous and normally distributed\n- Pearson correlation requires numeric variables; it‚Äôs not appropriate to compute it directly between categorical and numerical variables or between two categorical variables\n- Alternatives exist : \n\t- Spearman‚Äôs rho (for ordinal or ranked data)\n\t- Kendall's tau (for ordinal or ranked data)\n\t- Phi coefficient (for two binary variables)\n:::\n\n\n\n## Scenario\n\n- What do you think the correlation is between $X$ and $Y$?\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-12-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Compute correlation\n\n- $r=0.187$ (weak, positive correlation)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf |>\n  select(x, y) |>\n  correlate() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 √ó 3\n  term       x      y\n  <chr>  <dbl>  <dbl>\n1 x     NA      0.187\n2 y      0.187 NA    \n```\n\n\n:::\n:::\n\n\n\n## \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np2\n```\n\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-14-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Simpson‚Äôs Paradox\n\n- **Simpson‚Äôs Paradox** is a statistical phenomenon in which an association between two variables in an overall population appears, disappears, or reverses when the population is separated into subpopulations.\n \n\n![](https://raw.githubusercontent.com/simplystats/simplystats.github.io/master/_images/simpsons-paradox.gif)\n\n> [https://simplystatistics.org/posts/2017-08-08-code-for-my-educational-gifs/](https://simplystatistics.org/posts/2017-08-08-code-for-my-educational-gifs/)\n\n## Bill Length vs. Bill Depth\n\nUsing the `palmerpenguins` dataset:\n\n1. Make a scatterplot of **bill length** vs. **bill depth**\n2. Compute the **correlation** between the two variables\n3. Then, color the points by **species** and observe what changes\n\n![](https://www.natekratzer.com/posts/simpsons_paradox/culmen_depth.png)\n\n## iClicker Question\n\nWhat best describes the relationship between bill length and bill depth?\n\n- A. Negative correlation overall; still negative within each species  \n- B. Positive correlation overall; stronger within each species  \n- C. Negative correlation overall; positive within each species  \n- D. Minimal correlation overall or within groups\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-15-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Correlation ‚â† Causation\n\n- Just because two variables are correlated, does not mean one causes the other! \n- Correlation can also be coincidental or arise from a confounding variable (more to come later in the course).\n\n<div style=\"text-align: center;\">\n  <img src=\"https://gallery.mailchimp.com/0625ea07a42772bf4ec49dee5/images/e40ff83b-ae8f-439a-b745-2a7b3b5d215a.jpg\" style=\"width: 60%; max-width: 500px\">\n</div>\n\n<div style=\"text-align: center; font-size: 0.5em;\">\nSource: <a href=\"https://meltchocolates.com/newsletters/chocolate-makes-you-smart/\" target=\"_blank\">https://meltchocolates.com/newsletters/chocolate-makes-you-smart/</a>\n</div>\n\n\n## EDA and the Data Pipeline\n\n:::incremental\n- EDA choices are not just cosmetic; they can fundamentally alter your analysis!\n- What we filter, transform, and create in EDA directly shapes:\n  - What models we build\n  - How well they perform\n  - How we interpret results\n:::\n\n## In-class Exercise\n\n\n- You're working with data from a large tech company that wants to predict which employees are likely to leave within the next year.\n\n- The dataset includes:\n\t- `income`: current salary\n\t- `years_exp`: total years of experience\n\t- `postalcode`: employee home location\n\t- `left_company`: target variable (1 = left, 0 = stayed)\n\n- You‚Äôre doing EDA to prep for modeling `left_company`.\n\n\n## In-class Exercise\n\nGiven the EDA pipeline below, discuss with a neighbour:\n\n- Which steps might improve modeling?\n- Which could bias results or reduce generalizability?\n- What would you ask your team before proceeding?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# EDA pipeline in R\ndf <- raw_data |> \n  filter(income > 0) |>\n  mutate(\n    log_income = log(income),\n    seniority = years_exp > 5\n  ) |>\n  select(-postalcode)\n```\n:::\n\n\n\n## Diamonds Example \n\n- You're working with diamond pricing data (`diamonds` data from `ggplot2` pacakge). üíé\n- Goal: Predict the price of a diamond (`price`) using its carat weight (`carat`).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(diamonds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 10\n  carat cut       color clarity depth table price     x     y     z\n  <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n```\n\n\n:::\n:::\n\n\n\n## \n\n- Before diving into modeling, a colleague says:\n\n> *‚ÄúWe usually talk about diamond size in terms of groups XS-XL when discussing pricing with customers. Should we just model with those?‚Äù*\n\n- You now face a choice:\n\t- Model `carat` as a **continuous** variable  \n\t- Or bin `carat` into categories \n\n- What do you think the tradeoffs are?\n\n## \n\nLet's compare:\n\n- Model A: Continuous `carat`\n- Model B: Binned `carat_group`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf <- diamonds |>\n  select(price, carat) |>\n  mutate(\n    carat_group = cut(\n      carat,\n      breaks = c(0, 0.5, 1, 1.5, 2, 5),\n      labels = c(\"extra small\", \"small\", \"medium\", \"large\", \"extra large\"),\n      right = FALSE\n    )\n  ) |>\n  filter(!is.na(carat_group))\n```\n:::\n\n\n\n## \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-19-1.svg){fig-align='center'}\n:::\n:::\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf |>\n  ggplot(aes(x = carat_group, y = price)) +\n  geom_boxplot(fill=\"cornflowerblue\") +\n  labs(title = \"Diamond Price by Carat Group\") +\n\ttheme_minimal()\n```\n\n::: {.cell-output-display}\n![](eda-2_files/figure-revealjs/unnamed-chunk-20-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rsample)\nlibrary(tidymodels)\n\nset.seed(200)\ndiamond_split <- initial_split(df, prop = 0.75, strata = price)\ndiamond_train <- training(diamond_split)\ndiamond_test <- testing(diamond_split)\n\n# Model spec\nlm_spec <- linear_reg() |>\n  set_engine(\"lm\") |>\n  set_mode(\"regression\")\n\n# Recipe for Model A (carat continuous)\nlm_recipe_A <- recipe(price ~ carat, data = diamond_train)\n\n# Recipe for Model B (carat binned categories)\nlm_recipe_B <- recipe(price ~ carat_group, data = diamond_train) \n\n# Workflow for Model A\nlm_fit_A <- workflow() |>\n  add_recipe(lm_recipe_A) |>\n  add_model(lm_spec) |>\n  fit(data = diamond_train)\n\n# Workflow for Model B\nlm_fit_B <- workflow() |>\n  add_recipe(lm_recipe_B) |>\n  add_model(lm_spec) |>\n  fit(data = diamond_train)\n```\n:::\n\n\n\n## Fitted models\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(lm_fit_A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   -2250.      15.0     -150.       0\n2 carat          7745.      16.2      479.       0\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(lm_fit_B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 √ó 5\n  term                   estimate std.error statistic p.value\n  <chr>                     <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)                792.      13.5      58.5       0\n2 carat_groupsmall          1705.      19.3      88.3       0\n3 carat_groupmedium         5345.      20.9     256.        0\n4 carat_grouplarge         10081.      31.3     322.        0\n5 carat_groupextra large   14012.      41.1     341.        0\n```\n\n\n:::\n:::\n\n\n\n## RMSPE \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Model A (carat continuous)\nrmse_A <- lm_fit_A |>\n  predict(diamond_test) |>\n  bind_cols(diamond_test) |>\n  rmse(truth = price, estimate = .pred) |>\n  mutate(model = \"Carat continuous\")\n\n# Model B (carat binned)\nrmse_B <- lm_fit_B |>\n  predict(diamond_test) |>\n  bind_cols(diamond_test) |>\n  rmse(truth = price, estimate = .pred) |>\n  mutate(model = \"Carat binned\")\n\nrmse_results <- bind_rows(rmse_A, rmse_B) |>\n  select(model, .metric, .estimator, .estimate)\n\nprint(rmse_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 √ó 4\n  model            .metric .estimator .estimate\n  <chr>            <chr>   <chr>          <dbl>\n1 Carat continuous rmse    standard       1561.\n2 Carat binned     rmse    standard       1572.\n```\n\n\n:::\n:::\n\n\n\n## Discussion: Think, Pair, Share\n\n- Continuous or binned: which strategy would you choose and why?\n\n- What do we assume when we bin carat?\n\n- Could this influence pricing strategies?\n\n\n## iClicker Question\n\nIn your opinion, when is it most appropriate to split your data into training and test sets?\n\n- A. Before doing any EDA  \n- B. After EDA\n- C. Right before modeling  \n- D. I'm not sure\n\n## Why Split Before EDA?\n\n:::incremental\n- It is often recommended to split your data *before* performing EDA. \n- Analyzing the full dataset can leak information from the test set into the model or analysis. \n- If we tailor preprocessing, feature engineering or generate hypotheses using the whole dataset:\n  - It introduces data snooping bias.\n  - The model may overfit to patterns it shouldn't have access to.\n- Remember your test set should simulate unseen data!\n:::\n\n## It‚Äôs Not Always One-Size-Fits-All!\n\n:::incremental\n- The decision to split before or after EDA depends on your goals and the context.\n\n- Sometimes it makes sense to perform minimal checks on the full dataset to:\n  - Detect inconsistent data types\n  - Check for class imbalance\n  - Identify missing values or obvious data issues\n\n- If your goal is descriptive analysis (not predictive modeling or inference), a train/test split may not be necessary.\n\n- The key is to think critically about your objectives and the potential for data leakage.\n:::\n\n## Key Takeaways \n\n:::incremental\n- Correlation quantifies the strength and direction of relationships between variables\n\n- Pearson‚Äôs correlation captures only linear relationships and it may miss or misrepresent more complex patterns\n\n- Choices during EDA (e.g., filtering, transforming, and feature selection) shape your modeling outcomes\n\n- Splitting data before EDA helps prevent leakage and supports valid inference and model evaluation\n:::\n",
    "supporting": [
      "eda-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}