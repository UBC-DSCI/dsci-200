{
  "hash": "e1c54baccaad86a480e6bc94547434e6",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"13 - Data Privacy I\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n\n## {{< meta lecture >}} {.large background-image=\"img/smooths.png\" background-opacity=\"0.3\" background-size=\"50%\"}\n\n[DSCI 200]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 22 October 2025\n\n\n\n\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## Attribution\n<br><br>\n\n*This material is adapted from the following sources:* \n\n- [*Data Privacy Handbook*](https://utrechtuniversity.github.io/dataprivacyhandbook/)\n- [*DSCI 541: Privacy, Ethics, and Security at UBC*](https://ubc-mds.github.io/course-descriptions/DSCI_541_priv-eth-sec/)\n- [*STA 199: Introduction to Data Science and Statistical Thinking, Duke University*](https://sta199-s24.github.io/) \n\n<br><br>\n\n\n## What Are You Comfortable Sharing?\n\n\n\n::: column\nConsider different types of data:\n\n:::incremental\n- Your favorite type of music  \n- Your Instagram likes and follows  \n- Your e-mail  \n- Your name and DOB  \n- Your GPS location throughout the day \n- Your browsing history\n- Your private messages/DMs  \n:::\n:::\n\n::: column\n:::incremental\n- Discussion: \n1. Which of these data would you feel comfortable sharing with an app?  \n2. What questions would you want to ask before sharing this data?  \n3. What if it combined two or three pieces of information? \n:::\n:::\n\n## Learning Objectives\n\nBy the end of today's lesson, you should be able to:\n\n:::incremental\n- Understand key terms in data privacy, including PII, pseudonymization, and anonymization\n\n- Identify direct and indirect identifiers in sample data sets\n- Explain why de-identification is challenging and context-dependent\n- Apply basic de-identification techniques (e.g., suppression, top-coding, permutation) using R\n- Recognize the tradeoff between data utility and privacy risk\n:::\n\n\n## \n\n- Even something as simple as your Facebook \"likes\" can reveal a lot more than you think...\n- Researchers at Cambridge showed that algorithms could predict:\n\t- Sexual orientation with up to 88% accuracy\n\t- Race with 95% accuracy\n\t- Political affiliation with 85% accuracy\n- All from analyzing the pages and posts you \"liked\" (no profile bio or messages needed)!\n\n> [https://www.cam.ac.uk/research/news/digital-records-could-expose-intimate-details-and-personality-traits-of-millions](https://www.cam.ac.uk/research/news/digital-records-could-expose-intimate-details-and-personality-traits-of-millions)\n\n## What Happens to Your Data?\n\nEvery time you use an app, visit a website, click on a link, fill out a survey or even just scroll on your device, your data is being:\n\n:::incremental\n- Collected - What you click, search, watch, like or buy  \n- Analyzed - Used to predict your behaviour, interests or identity  \n- Shared or Sold - Passed to advertisers, data brokers or other companies  \n:::\n\n![](https://images.unsplash.com/photo-1579869847557-1f67382cc158?w=900&auto=format&fit=crop&q=60&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxzZWFyY2h8MXx8c29jaWFsJTIwbWVkaWF8ZW58MHx8MHx8fDI%3D)\n\n---\n\n## Why Does This Matter?\n\n:::incremental\n- You may be targeted with ads, content and potentially misinformation  \n- You could be judged or profiled based on your data (even if it’s not accurate) \n- You rarely know who has your data (or what they’re doing with it) \n<br></br>\n<br></br>\n- So what does this mean for us? Let’s explore how data can be used, what makes certain information sensitive and why it matters.\n:::\n\n## Personally Identifiable Information (PII)\n\n- PII refers to any data that can be used to identify a specific individual.\n- Direct identifiers: These clearly and uniquely point to a person.\n\t- Examples: name, social security number, patient ID\n- Indirect identifiers: These don’t identify someone on their own, but could when combined.\n\t- Examples: age, DOB, postal code, race, sex\n\n## Personal Data\n\nData can be identifiable when:\n\n:::incremental\n- They contain directly identifying information.\n- It's possible to single out an individual\n- It's possible to infer information about an individual based on information in your dataset\n- It's possible to link records relating to an individual.\n- De-identification is still reversible.\n:::\n\n## Scenario: Can This Data Identify You?\n\nA fitness app shares anonymized data with researchers. The dataset includes:\n\n- Step count per day\n- General location (postal code)\n- Age \n- Time of day the user exercises\n- Health conditions\n\nSeparately, a publicly available dataset includes information from a local running club: names, age groups and 5K race times.\n\n## The Mosaic Effect\n\n- The \"Mosaic Effect\" can happens when separate pieces of data, which alone don’t identify anyone, are combined from different sources to reveal personal information or identify an individual.\n\n- In 2000, 87% of the United States population was found to be identifiable using a combination of their ZIP code, gender and date of birth.\n\n![](https://images.unsplash.com/photo-1622227920933-7fcd7377703f?q=80&w=2940&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D)\n\n> [https://dataprivacylab.org/projects/identifiability/paper1.pdf](https://dataprivacylab.org/projects/identifiability/paper1.pdf)\n\n## Pseudonymization and Anonymization\n\n- Pseudonymisation and a nonymisation are techniques to de-identify personal data\n- Goal: reduce linkability of data to individuals\n- We will now define each of these terms \n\n\n## Pseudonymization\n\n::: incremental\n- Reduces linkability of data to individuals \n- Data cannot identify individuals *without additional information* \n- Often done by replacing direct identifiers with pseudonyms\n- Link between real identifiers and pseudonyms is stored separately\n- Re-identification remains possible!\n:::\n\n\n## Anonymization\n\n::: incremental\n- Data are anonymized when no individual is identifiable (directly or indirectly)\n- This applies even to the data controller\n- Fully anonymized data are no longer personal data\n- Anonymisation is difficult to achieve in practice\n:::\n\n## Identifiability Spectrum\n\n- Identifiability is a spectrum \n- More de-identified data = closer to anonymized\n- Lower identifiability = lower re-identification risk\n\n\n![](https://www.kdnuggets.com/wp-content/uploads/spectrum_data_privacy_KD.png)\n\n> [https://www.kdnuggets.com/2020/08/anonymous-anonymized-data.html](https://www.kdnuggets.com/2020/08/anonymous-anonymized-data.html)\n\n## When Are Data Truly anonymous?\n\n- Only if re-identification would require *unreasonable effort* (factors include cost, time and available technology)\n- Data are not anonymous if:\n\n::: incremental\n- Direct identifiers are present\n- Individuals can be singled out from a group\n- Re-identification possible via linking datasets (mosaic effect)\n- Inference about identity is possible (e.g., through different variables)\n- De-identification can be reversed\n:::\n\n## Context Matters\n\n- Whether data are anonymous depends on:\n  - The context of the research\n  - Available external information\n  - Future data uses\n\n## De-identification Techniques\n\nTechniques to deidentify your data include:\n\n- Suppression\n- Generalization\n- Replacement\n- Top- and bottom coding\n- Adding noise\n- Permutation\n\nWe will talk about each of these techniques individually. \n\n## \n\nFirst, let's generate some data we can use to help illustrate these concepts. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndf <- tibble(\n  name = c(\"Joel Miller\", \"Ellie Williams\", \"Tommy Miller\", \"Abby Anderson\"),\n  age = c(52, 19, 48, 28),\n  height_cm = c(182, 160, 185,173) \n)\n\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  name             age height_cm\n  <chr>          <dbl>     <dbl>\n1 Joel Miller       52       182\n2 Ellie Williams    19       160\n3 Tommy Miller      48       185\n4 Abby Anderson     28       173\n```\n\n\n:::\n:::\n\n\n\n\n\n## Suppression\n\n::: incremental\n- Remove entire variables, values or records\n- Used to eliminate highly identifying or unnecessary data\n- Examples:\n  - Names, contact details, social security numbers\n  - GPS metadata, IP addresses, neuroimaging facial features\n  - Outliers or unique participants\n:::\n\n## Suppression Example\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_suppressed <- df %>%\n  select(-name)\n\ndf_suppressed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n    age height_cm\n  <dbl>     <dbl>\n1    52       182\n2    19       160\n3    48       185\n4    28       173\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Generalization\n\n::: incremental\n- Reduces detail or granularity in the data\n- Makes individuals harder to single out\n- Examples:\n  - Convert date of birth to age, or group into ranges\n  - Replace address with town or region\n  - Recategorise rare labels into “other” or “missing”\n  - Abstract people or places in qualitative data (e.g., “Bob” to “[colleague]”)\n:::\n\n## Generalization Example\n\nHere we will show an example of generalization on the `age` column: \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_generalized <- df |>\n  mutate(age_group = case_when(\n    age < 30 ~ \"under 30\",\n    TRUE     ~ \"30+\"\n  ))|>\n  select(-age)\n\ndf_generalized\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  name           height_cm age_group\n  <chr>              <dbl> <chr>    \n1 Joel Miller          182 30+      \n2 Ellie Williams       160 under 30 \n3 Tommy Miller         185 30+      \n4 Abby Anderson        173 under 30 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Replacement\n\n::: incremental\n- Swap identifying info with less informative alternatives\n- Examples:\n  - Use pseudonyms for names (with securely stored keyfile)\n  - Replace with placeholders (e.g., “[redacted]”)\n  - Rounding numeric values\n:::\n\n## Creating Pseudonyms\n\n::: incremental\n- Pseudonyms should reveal nothing about the subject\n- Good pseudonyms:\n  - Are random or meaningless strings/numbers\n  - Are securely managed (e.g., encrypted keyfile)\n- Can be generated using tools in Excel, R, Python, SPSS\n:::\n\n## Replacement with Pseudonyms\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_pseudonymized <- df |>\n  mutate(pseudonym = paste0(\"ID\", row_number())) |>\n  select(pseudonym, everything(), -name)\n\ndf_pseudonymized\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  pseudonym   age height_cm\n  <chr>     <dbl>     <dbl>\n1 ID1          52       182\n2 ID2          19       160\n3 ID3          48       185\n4 ID4          28       173\n```\n\n\n:::\n:::\n\n\n\n\n\n## Hashing \n\n- Hashing converts names into fixed-length, irreversible strings.\n- Unlike pseudonyms, hashed values cannot be easily reversed.\n- In R, we can use the `digest` package (and function) to hash. \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(digest) \n\ndf_hashed <- df |>\n  mutate(name_hash = digest(name)) |>\n  select(name_hash, everything(), -name)\n\ndf_hashed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  name_hash                          age height_cm\n  <chr>                            <dbl>     <dbl>\n1 83ecf15bd80bd4b7c46e1f2717c08cd0    52       182\n2 83ecf15bd80bd4b7c46e1f2717c08cd0    19       160\n3 83ecf15bd80bd4b7c46e1f2717c08cd0    48       185\n4 83ecf15bd80bd4b7c46e1f2717c08cd0    28       173\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Top- and Bottom-Coding\n\n::: incremental\n- Limits extreme values in quantitative data\n- Recode all values above or below a threshold\n- Example: all incomes above \\$150,000 become \\$150,000\n- Preserves much of the dataset, but distorts distribution tails\n:::\n\n## Top-coding example\n\n- Consider 6ft (182.88cm) is considered our maximum height threshold.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_top_coded <- df |>\n  mutate(height_cm = if_else(height_cm > 182.88, 182.88, height_cm))\n\ndf_top_coded\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  name             age height_cm\n  <chr>          <dbl>     <dbl>\n1 Joel Miller       52      182 \n2 Ellie Williams    19      160 \n3 Tommy Miller      48      183.\n4 Abby Anderson     28      173 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Adding Noise\n\n- Introduces randomness to protect sensitive info\n- Examples:\n  - Add a small random amount to numeric values\n  - Blur images or alter voices\n  - Use differential privacy algorithms (advanced)\n\n## Adding Noise to Height\n\nThis adds random noise to the height variable from a normal distribution ($\\mu=0$, $\\sigma=2$), reducing exact re-identification risk.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(200) \n\ndf_noisy <- df |>\n  mutate(height_cm_noisy = height_cm + rnorm(n(), mean = 0, sd = 2)) |>\n\tselect(-height_cm)\n\ndf_noisy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  name             age height_cm_noisy\n  <chr>          <dbl>           <dbl>\n1 Joel Miller       52            182.\n2 Ellie Williams    19            160.\n3 Tommy Miller      48            186.\n4 Abby Anderson     28            174.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Permutation\n\n::: incremental\n- Swap values between individuals\n- Makes linking variables across a record more difficult\n- Maintains distributions, but breaks correlations\n- Can limit the types of analyses possible\n:::\n\n## Permutation of Height Values\n\nHere, the `height_cm` values are shuffled between individuals, preserving the overall distribution but breaking the link between person and value.\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(200)\n\ndf_permuted <- df |>\n  mutate(height_cm_permuted = sample(height_cm)) |>\n\tselect(-height_cm)\n\ndf_permuted\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  name             age height_cm_permuted\n  <chr>          <dbl>              <dbl>\n1 Joel Miller       52                160\n2 Ellie Williams    19                173\n3 Tommy Miller      48                182\n4 Abby Anderson     28                185\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Privacy vs. Utility Tradeoff\n\n![](https://www.researchgate.net/publication/357987903/figure/fig1/AS:1114625957470208@1642758945429/Trade-off-between-privacy-level-and-utility-level-of-data.png)\n\n>[https://www.researchgate.net/figure/Trade-off-between-privacy-level-and-utility-level-of-data_fig1_357987903](https://www.researchgate.net/figure/Trade-off-between-privacy-level-and-utility-level-of-data_fig1_357987903)\n\n## Key Takeaways\n\n- Data exists on a spectrum of identifiability \n- Even seemingly anonymous data can often be re-identified (e.g., mosaic effect)\n- Different techniques offer varying levels of protection and utility\n- Context, external data and technological capabilities all affect re-identification risk\n- Responsible data handling requires both technical skill and ethical awareness",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}