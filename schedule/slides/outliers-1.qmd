---
lecture: "Outliers I"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}

## Attribution
<br>

- Some examples and references are from [*Challenges of cellwise outliers*, by Raymaekers and Rousseeuw](https://www.sciencedirect.com/science/article/pii/S2452306224000078) 

- [*Robust Statistics*, by Maronna, Martin, Yohai, Salibian-Barrera](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119214656)

<br>

*In memory of my mentor and friend*

Professor Ruben Zamar (1949-2023)

who contributed greatly to Robust Statistics and beyond ...

<br><br>

## Learning Objectives {.scrollable}

<br>
By the end of this lesson, you will be able to:

:::incremental
- Determine potential reasons why data contain outliers.
- Compare outliers with extreme values generated from heavy-tailed distributions.
- Recognize different types of outliers in a data set (e.g., casewise versus cellwise outliers).
- Use different methods to detect outliers in a data set and understand differences in the results.
- Justify and apply strategies for managing outliers in the data, including transformations and data imputation.
- Write a computer script to evaluate the impact that outliers can have on subsequent analyses through simulation.
- Reflect on the consequences with regards to the conclusions of the chosen method.
- Recognize the importance of utilizing domain knowledge when handling outliers.
:::

## What is an outlier?

![](img/pexels-outlier.png)

## Outliers

::: {.callout-note title="Definition: Outlier"}
An **outlier** is an observation that deviates from the bulk of the data, an atypical observation. 
:::

#### Outliers may arise from:

- errors when collecting or processing data

- values generated from a different distribution

- rare cases which may carry valuable information

<br>

As with missing values, outliers require careful diagnosis and appropriate handling before analysis.

## Casewise *vs* Cellwise

![](img/Case_Cell.png)

## Casewise 

::: {.columns}

::: {.column width="45%"}
![](img/Case.png)
:::

::: {.column width="55%" .v-center}

<br><br>
In the 1960s Tukey and Huber introduced the *casewise* (aka rowwise) contamination model 

- Treats entire rows as contaminated and coming from a different distribution, even if only values of some variables are unusual

- Assumes that less than 50% of the cases (objects) are contaminated

:::
:::

## Cellwise

::: {.columns}

::: {.column width="45%"}
![](img/Cell.png)
:::

::: {.column width="55%" .v-center}

<br><br>

In 2009, Alqallaf, Van Aelst, Yohai and Zamar introduced the *cellwise* contamination model

- Only individual cells are contaminated, with values coming from a different distribution

- Any case (row) may contain some contaminated cells

- More realistic for high-dimensional data

:::
:::

## Glass data

![](img/glass.png)

Data: n = 180 archeological glass spectra with d = 750 wavelengths ([Detecting Deviating Data Cells](https://www.tandfonline.com/doi/figure/10.1080/00401706.2017.1340909?scroll=top&needAccess=true), Rousseeuw & Van Den Bossche, *Technometrics* 2018)

## Univariate estimators

Before analyzing multivariate datasets, we first need to learn how to identify and handle outlying values in a single variable.

Let's look at the values of the wavelength V169 in this data

```{r echo=FALSE}
library(robustbase)
library(cellWise)
library(ggplot2)
data(data_glass)
df <- data.frame(x = data_glass[,"V169"])

ggplot(df, aes(x = x)) +
  geom_histogram(bins = 40, fill = "grey80", color = "white") +
  geom_vline(aes(xintercept = mean(x), color = "Mean"), linewidth = 1) +
  geom_vline(aes(xintercept = median(x), color = "Median"), linewidth = 1) +
  scale_color_manual(values = c(Mean = "red", Median = "blue")) +
  labs(title = "Wavelet Glass Spectra",
       x = "Coefficient value",
       y = "Count",
       color = NULL) +
  theme_minimal()
```

## Robust estimators

- Classical estimators can be highly distorted by outliers

- Robust estimators are needed to capture the bulk of the data

- Robust estimators are also needed to flag outliers

<br>

| Statistic | Estimate |
|-----------|----------|
| Mean      | `r round(mean(df$x),2)` |
| Median    | `r round(median(df$x),2)` |
| SD        | `r round(sd(df$x),2)` |
| MAD       | `r round(mad(df$x),2)` |

<br>

#### The mean and SD are very sensitive to the outliers in the data

## Mean ($\bar{X}$) *vs* Median ($\tilde{X}$)

If the median is less affected by extreme values, why not using it as a default?

- if the data contain outliers, the median is more resistant

- but if the data does not contain outliers, it is less efficient (larger standard error!)

#### If data is Normally distributed:

$$SE(\bar{X}) = \frac{\sigma}{\sqrt{n}}$$

$$SE(\tilde{X})= \sqrt{\frac{\pi}{2}}\frac{\sigma}{\sqrt{n}}$$

Let's examine these two points by simulation!

## Simulation design

### Clean data

- Generate a sample of size 100 from a Normal distribution with mean 0 and standard deviation 1

- Compute the mean and the median

- Repeat 10000 times

- Summarize their sampling distributions


##

```{r}
set.seed(200)

sim_stats <- replicate(10000, {
  x <- rnorm(100)
  c(mean = mean(x),
    median = median(x))
})

sim_stats <- t(sim_stats) |> as.data.frame()

sd(sim_stats$mean)
1/sqrt(1000)

sd(sim_stats$median)
sqrt(pi/2)*0.001
```

##

```{r echo=FALSE}
set.seed(200)

sim_stats <- replicate(10000, {
  x <- rnorm(100)
  c(mean = mean(x),
    median = median(x))
})

sim_stats <- t(sim_stats) |> as.data.frame()

ggplot(sim_stats) +
  geom_histogram(aes(x = mean, fill = "Mean"),
                 bins = 50, alpha = 0.35, color = NA) +
  geom_histogram(aes(x = median, fill = "Median"),
                 bins = 50, alpha = 0.35, color = NA) +
  scale_fill_manual(values = c(Mean = "#E41A1C",
                               Median = "#377EB8")) +
  labs(title = "Sampling distributions: mean vs median (Normal data)",
       x = "Statistic value",
       y = "Frequency",
       fill = NULL) +
  theme_minimal(base_size = 14)
```

## Simulation design

### Contaminated data

- Generate a sample of size 100, with 90% from $\mathcal{N}(0,1)$ and 10% from $\mathcal{N}(0,10)$

- Compute the mean and the median

- Repeat 10000 times

- Summarize their sampling distributions

$$\text{Var}(\bar{X}) = \frac{(1-\varepsilon)\sigma^2_1 + \varepsilon \sigma_2^2}{n}$$

$$\mathrm{Var}(\tilde{X}) \approx \frac{\pi}{2n}\;
\frac{1}{\left(\frac{1-\varepsilon}{\sigma_1}+\frac{\varepsilon}{\sigma_2}\right)^2}$$

##

```{r}
rcont_norm <- function(n, eps = 0.10, mu0 = 0, 
                       sigma0 = 1, sigma1 = 10) {
  x <- rnorm(n, mean = mu0, sd = sigma0)
  
  k <- ceiling(eps * n)              
  if (k > 0) {
    idx <- sample(1:n, k)
    x[idx] <- rnorm(k, mean = mu0, sd = sigma1)
  }
  x
}

set.seed(200)

sim_stats <- replicate(10000, {
  x <- rcont_norm(100)
  c(mean = mean(x),
    median = median(x))
})

sim_stats <- t(sim_stats) |> as.data.frame()

sd(sim_stats$mean)
sd(sim_stats$median)
dd <- (.9/1 + .1/10)^2
sqrt(pi/(2*100*dd))
```

## Key Takeaways

<br>

- Outliers can occur in all variables of an observation or only in certain variables

- Cellwise paradigm is better is more flexible for high-dimensional data

- Classical estimators can be optimal but very sensitive to outliers

- Median is less efficient (≈ 1.57× variance) but more stable under contamination

- Robustness = trade-off with efficiency



